---
title: "Regression Case Study Project"
output: html_notebook
author: "Agus Nur Hidayat"
---

### Preparing packages and their dependenciesn

```{r}
install.packages("lazyeval")
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("mlbench")
```

### Loading libraries

```{r echo=T}
library(caret)
library(mlbench)
library(corrplot)
```

### Prepare the dataset

#### Loading the dataset

```{r echo=TRUE}
# attach the BostonHousing dataset
data(BostonHousing)
```

#### Create a validation dataset

```{r}
# Split out validation dataset
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(7)
validationIndex = createDataPartition(BostonHousing$medv, p=0.80, list=FALSE)
# select 20% of the data for validation
validation = BostonHousing[-validationIndex,]
# use the remaining 80% of data to training and testing the models
dataset = BostonHousing[validationIndex,]
```

### Analyze the dataset

#### Descriptive statistics

```{r}
# dimensions of dataset
dim(dataset)
```

```{r}
# list types for each variable
sapply(dataset, class)
```

```{r}
# take a peek at the first 20 rows of the data
head(dataset, n=5)
```

```{r}
# summarize variable distributions
summary(dataset)
```

```{r}
# convert chas to a numeric variable
dataset[,4] = as.numeric(as.character(dataset[,4]))
```

```{r}
# correlation between all of the numeric variables
cor(dataset[,1:13])
```

#### Visualizing the dataset

##### Univariate plots

```{r}
# histograms each variable
par(mfrow=c(2,7))
for(i in 1:13) {
  hist(dataset[,i], main=names(dataset)[i])
}
```

```{r}
# density plot for each variable
par(mfrow=c(2,7))
for(i in 1:13) {
  plot(density(dataset[,i]), main=names(dataset)[i])
}
```

```{r}
# boxplots for each variable
par(mfrow=c(2,7))
for(i in 1:13) {
  boxplot(dataset[,i], main=names(dataset)[i])
}
```

##### Multivariate plots

```{r}
# scatter plot matrix
pairs(dataset[,1:13])
```

```{r}
# correlation plot
correlations = cor(dataset[,1:13])
corrplot(correlations, method="circle")
```

### Evaluating algorithms

#### Baseline

```{r}
# Run algorithms using 10-fold cross validation
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "RMSE"
```

```{r}
# LM
set.seed(7)
fit.lm = train(medv~., data=dataset, method="lm", metric=metric, preProc=c("center",
    "scale"), trControl=trainControl)
# GLM
set.seed(7)
fit.glm = train(medv~., data=dataset, method="glm", metric=metric, preProc=c("center",
    "scale"), trControl=trainControl)
# GLMNET
set.seed(7)
fit.glmnet = train(medv~., data=dataset, method="glmnet", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(7)
fit.svm = train(medv~., data=dataset, method="svmRadial", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
# CART
set.seed(7)
grid = expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart = train(medv~., data=dataset, method="rpart", metric=metric, tuneGrid=grid,
    preProc=c("center", "scale"), trControl=trainControl)
# KNN
set.seed(7)
fit.knn = train(medv~., data=dataset, method="knn", metric=metric, preProc=c("center",
"scale"), trControl=trainControl)
```

```{r}
# Compare algorithms
results = resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm,
    CART=fit.cart, KNN=fit.knn))
summary(results)
dotplot(results)
```

#### Feature Selection

```{r}
# remove correlated variables
# find variables that are highly correlated
set.seed(7)
cutoff = 0.70
correlations = cor(dataset[,1:13])
highlyCorrelated = findCorrelation(correlations, cutoff=cutoff)
for (value in highlyCorrelated) {
  print(names(dataset)[value])
}
# create a new dataset without highly correlated features
datasetFeatures = dataset[,-highlyCorrelated]
dim(datasetFeatures)
```

```{r}
# Run algorithms using 10-fold cross validation
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "RMSE"
```

```{r}
# lm
set.seed(7)
fit.lm = train(medv~., data=datasetFeatures, method="lm", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
# GLM
set.seed(7)
fit.glm = train(medv~., data=datasetFeatures, method="glm", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(7)
fit.glmnet = train(medv~., data=datasetFeatures, method="glmnet", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(7)
fit.svm = train(medv~., data=datasetFeatures, method="svmRadial", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
# CART
set.seed(7)
grid = expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart = train(medv~., data=datasetFeatures, method="rpart", metric=metric,
    tuneGrid=grid, preProc=c("center", "scale"), trControl=trainControl)
# KNN
set.seed(7)
fit.knn = train(medv~., data=datasetFeatures, method="knn", metric=metric,
    preProc=c("center", "scale"), trControl=trainControl)
```

```{r}
# Compare algorithms
feature_results = resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm,
    CART=fit.cart, KNN=fit.knn))
summary(feature_results)
dotplot(feature_results)
```

#### Box-Cox Transform

```{r}
# Run algorithms using 10-fold cross validation
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "RMSE"
```

```{r}
# lm
set.seed(7)
fit.lm = train(medv~., data=dataset, method="lm", metric=metric, preProc=c("center",
    "scale", "BoxCox"), trControl=trainControl)
# GLM
set.seed(7)
fit.glm = train(medv~., data=dataset, method="glm", metric=metric, preProc=c("center",
    "scale", "BoxCox"), trControl=trainControl)
# GLMNET
set.seed(7)
fit.glmnet = train(medv~., data=dataset, method="glmnet", metric=metric,
    preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
# SVM
set.seed(7)
fit.svm = train(medv~., data=dataset, method="svmRadial", metric=metric,
    preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
# CART
set.seed(7)
grid = expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart = train(medv~., data=dataset, method="rpart", metric=metric, tuneGrid=grid,
    preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
# KNN
set.seed(7)
fit.knn = train(medv~., data=dataset, method="knn", metric=metric, preProc=c("center",
    "scale", "BoxCox"), trControl=trainControl)
```

```{r}
# Compare algorithms
transformResults = resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm,
    CART=fit.cart, KNN=fit.knn))
summary(transformResults)
dotplot(transformResults)
```

### Improve results

#### Tuning

```{r}
# tune SVM sigma and C parametres
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "RMSE"
set.seed(7)
grid = expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10, by=1))
fit.svm = train(medv~., data=dataset, method="svmRadial", metric=metric, tuneGrid=grid,
    preProc=c("BoxCox"), trControl=trainControl)
print(fit.svm)
plot(fit.svm)
```

#### Ensemble

```{r}
# try ensembles
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "RMSE"
# Random Forest
set.seed(seed)
fit.rf = train(medv~., data=dataset, method="rf", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm = train(medv~., data=dataset, method="gbm", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl, verbose=FALSE)
# Cubist
set.seed(seed)
fit.cubist = train(medv~., data=dataset, method="cubist", metric=metric,
    preProc=c("BoxCox"), trControl=trainControl)
```

```{r}
# Compare algorithms
ensembleResults = resamples(list(RF=fit.rf, GBM=fit.gbm, CUBIST=fit.cubist))
summary(ensembleResults)
dotplot(ensembleResults)
```

```{r}
# look at parameters used for Cubist
print(fit.cubist)
```

```{r}
# Tune the Cubist algorithm
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "RMSE"
set.seed(7)
grid = expand.grid(.committees=seq(15, 25, by=1), .neighbors=c(3, 5, 7))
tune.cubist = train(medv~., data=dataset, method="cubist", metric=metric,
    preProc=c("BoxCox"), tuneGrid=grid, trControl=trainControl)
print(tune.cubist)
plot(tune.cubist)
```

### Finalizing model

```{r}
# prepare the data transform using training data
set.seed(7)
x = dataset[,1:13]
y = dataset[,14]
preprocessParams = preProcess(x, method=c("BoxCox"))
transX = predict(preprocessParams, x)
# train the final model
finalModel = cubist(x=transX, y=y, committees=18)
summary(finalModel)
```

```{r}
# transform the validation dataset
set.seed(7)
valX = validation[,1:13]
trans_valX = predict(preprocessParams, valX)
valY = validation[,14]
# use final model to make predictions on the validation dataset
predictions = predict(finalModel, newdata=trans_valX, neighbors=3)
# calculate RMSE
rmse = sqrt(mean((predictions-valY)^2))
print(rmse)
```

