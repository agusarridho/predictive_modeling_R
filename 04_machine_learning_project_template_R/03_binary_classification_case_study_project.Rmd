---
title: "Binary Classification Case Study Project"
output: html_notebook
author: "Agus Nur Hidayat"
---

### Preparing packages and their dependenciesn

```{r}
install.packages("lazyeval")
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("mlbench")
```

### Loading libraries

```{r echo=T}
library(caret)
library(mlbench)
```

#### Prepare the dataset

##### Loading the dataset

```{r echo=TRUE}
# Load data
data(BreastCancer)
```

##### Create a validation dataset

```{r}
# Split out validation dataset
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(7)
validationIndex = createDataPartition(BreastCancer$Class, p=0.80, list=FALSE)
# select 20% of the data for validation
validation = BreastCancer[-validationIndex,]
# use the remaining 80% of data to training and testing the models
dataset = BreastCancer[validationIndex,]
```

#### Analyzing the dataset

##### Descriptive statistics

```{r}
# dimensions of dataaset
dim(dataset)
```

```{r}
# take a peek at the first 20 rows of the data
head(dataset, n=20)
```

```{r}
# types
sapply(dataset, class)
```

```{r}
# Remove redundant variable Id
dataset = dataset[,-1]
# convert input values to numeric
for(i in 1:9) {
 dataset[,i] = as.numeric(as.character(dataset[,i]))
}
```

```{r}
# summary
summary(dataset)
```

```{r}
# class distribution
cbind(freq=table(dataset$Class), percentage=prop.table(table(dataset$Class))*100)
```

```{r}
# summarize correlations between input variables
complete_cases = complete.cases(dataset)
cor(dataset[complete_cases,1:9])
```

##### Visualizing the dataset

###### Univariate plots

```{r}
# histograms each variable
par(mfrow=c(3,3))
for(i in 1:9) {
  hist(dataset[,i], main=names(dataset)[i])
}
```

```{r}
# density plot for each variable
par(mfrow=c(3,3))
complete_cases = complete.cases(dataset)
for(i in 1:9) {
  plot(density(dataset[complete_cases,i]), main=names(dataset)[i])
}
```

```{r}
# boxplots for each variable
par(mfrow=c(3,3))
for(i in 1:9) {
  boxplot(dataset[,i], main=names(dataset)[i])
}
```

###### Multivariate plots

```{r}
# scatter plot matrix
jittered_x = sapply(dataset[,1:9], jitter)
pairs(jittered_x, names(dataset[,1:9]), col=dataset$Class)
```

```{r}
# bar plots of each variable by class
par(mfrow=c(3,3))
for(i in 1:9) {
 barplot(table(dataset$Class,dataset[,i]), main=names(dataset)[i],
     legend.text=unique(dataset$Class))
}             
```

#### Evaluating algorithms

##### Baseline

```{r}
# 10-fold cross validation with 3 repeats
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
```

```{r}
# LG
set.seed(7)
fit.glm = train(Class~., data=dataset, method="glm", metric=metric, trControl=trainControl)
# LDA
set.seed(7)
fit.lda = train(Class~., data=dataset, method="lda", metric=metric, trControl=trainControl)
# GLMNET
set.seed(7)
fit.glmnet = train(Class~., data=dataset, method="glmnet", metric=metric,
    trControl=trainControl)
# KNN
set.seed(7)
fit.knn = train(Class~., data=dataset, method="knn", metric=metric, trControl=trainControl)
# CART
set.seed(7)
fit.cart = train(Class~., data=dataset, method="rpart", metric=metric,
    trControl=trainControl)
# Naive Bayes
set.seed(7)
fit.nb = train(Class~., data=dataset, method="nb", metric=metric, trControl=trainControl)
# SVM
set.seed(7)
fit.svm = train(Class~., data=dataset, method="svmRadial", metric=metric,
    trControl=trainControl)
```

```{r}
# Compare algorithms
results = resamples(list(LG=fit.glm, LDA=fit.lda, GLMNET=fit.glmnet, KNN=fit.knn,
    CART=fit.cart, NB=fit.nb, SVM=fit.svm))
summary(results)
dotplot(results)
```

##### Transform

```{r}
# 10-fold cross validation with 3 repeats
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
```

```{r}
# LG
set.seed(7)
fit.glm = train(Class~., data=dataset, method="glm", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
# LDA
set.seed(7)
fit.lda = train(Class~., data=dataset, method="lda", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
# GLMNET
set.seed(7)
fit.glmnet = train(Class~., data=dataset, method="glmnet", metric=metric,
    preProc=c("BoxCox"), trControl=trainControl)
# KNN
set.seed(7)
fit.knn = train(Class~., data=dataset, method="knn", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
# CART
set.seed(7)
fit.cart = train(Class~., data=dataset, method="rpart", metric=metric,
    preProc=c("BoxCox"), trControl=trainControl)
# Naive Bayes
set.seed(7)
fit.nb = train(Class~., data=dataset, method="nb", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
# SVM
set.seed(7)
fit.svm = train(Class~., data=dataset, method="svmRadial", metric=metric,
    preProc=c("BoxCox"), trControl=trainControl)
```

```{r}
# Compare algorithms
transformResults = resamples(list(LG=fit.glm, LDA=fit.lda, GLMNET=fit.glmnet, KNN=fit.knn,
    CART=fit.cart, NB=fit.nb, SVM=fit.svm))
summary(transformResults)
dotplot(transformResults)
```

### Improve results

#### Tuning

```{r}
# tuning SVM
# 10-fold cross validation with 3 repeats
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
set.seed(7)
grid = expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10, by=1))
fit.svm = train(Class~., data=dataset, method="svmRadial", metric=metric, tuneGrid=grid,
    preProc=c("BoxCox"), trControl=trainControl)
print(fit.svm)
plot(fit.svm)
```

```{r}
# tuning kNN
# 10-fold cross validation with 3 repeats
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
set.seed(7)
grid = expand.grid(.k=seq(1,20,by=1))
fit.knn = train(Class~., data=dataset, method="knn", metric=metric, tuneGrid=grid,
    preProc=c("BoxCox"), trControl=trainControl)
print(fit.knn)
plot(fit.knn)
```

#### Ensemble

```{r}
# 10-fold cross validation with 3 repeats
trainControl = trainControl(method="repeatedcv", number=10, repeats=3)
metric = "Accuracy"
```

```{r}
# Bagged CART
set.seed(7)
fit.treebag = train(Class~., data=dataset, method="treebag", metric=metric,
    trControl=trainControl)
# Random Forest
set.seed(7)
fit.rf = train(Class~., data=dataset, method="rf", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
# Stochastic Gradient Boosting
set.seed(7)
fit.gbm = train(Class~., data=dataset, method="gbm", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl, verbose=FALSE)
# C5.0
set.seed(7)
fit.c50 = train(Class~., data=dataset, method="C5.0", metric=metric, preProc=c("BoxCox"),
    trControl=trainControl)
```

```{r}
# Compare results
ensembleResults = resamples(list(BAG=fit.treebag, RF=fit.rf, GBM=fit.gbm, C50=fit.c50))
summary(ensembleResults)
dotplot(ensembleResults)
```

### Finalizing model

```{r}
# prepare parameters for data transform
set.seed(7)
datasetNoMissing = dataset[complete.cases(dataset),]
x = datasetNoMissing[,1:9]
preprocessParams = preProcess(x, method=c("BoxCox"))
x = predict(preprocessParams, x)
```

```{r}
# prepare the validation dataset
set.seed(7)
# remove id column
validation = validation[,-1]
# remove missing values (not allowed in this implementation of knn)
validation = validation[complete.cases(validation),]
# convert to numeric
for(i in 1:9) {
  validation[,i] = as.numeric(as.character(validation[,i]))
}
# transform the validation dataset
validationX = predict(preprocessParams, validation[,1:9])
```

```{r}
# make predictions
set.seed(7)
predictions = knn3Train(x, validationX, datasetNoMissing$Class, k=9, prob=FALSE)
confusionMatrix(predictions, validation$Class)
```

